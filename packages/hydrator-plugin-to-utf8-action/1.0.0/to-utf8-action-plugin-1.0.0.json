{
  "parents": [
    "system:cdap-data-pipeline[4.0.0,10.0.0-SNAPSHOT)",
    "system:cdap-data-streams[4.0.0,10.0.0-SNAPSHOT)"
  ],
  "properties": {
    "widgets.ToUTF8-action": "{\"metadata\":{\"spec-version\":\"1.4\"},\"configuration-groups\":[{\"label\":\"Configuration Options\",\"properties\":[{\"widget-type\":\"textbox\",\"label\":\"Source Path\",\"name\":\"sourceFilePath\",\"widget-attributes\":{\"placeholder\":\"E.g. /tmp/file.dat or /tmp/folder/\"}},{\"widget-type\":\"textbox\",\"label\":\"Destination Path\",\"name\":\"destFilePath\",\"widget-attributes\":{\"placeholder\":\"E.g. /tmp/file.utf8.dat or /tmp/utf8-folder/\"}},{\"widget-type\":\"textbox\",\"label\":\"File Regular Expression\",\"name\":\"fileRegex\",\"widget-attributes\":{\"placeholder\":\"E.g. .*\\\\.dat\"}},{\"widget-type\":\"textbox\",\"label\":\"Character Set\",\"name\":\"charset\",\"widget-attributes\":{\"placeholder\":\"E.g. ISO-8859-1\"}},{\"widget-type\":\"select\",\"label\":\"Continue Processing If There Are Errors?\",\"name\":\"continueOnError\",\"widget-attributes\":{\"values\":[\"true\",\"false\"],\"default\":\"false\"}}]}],\"outputs\":[]}",
    "doc.ToUTF8-action": "# To UTF-8 Action\n\n\nDescription\n-----------\nThe To UTF-8 Action is used to convert files created in other character sets\ninto UTF-8 format so that they can be processed using standard Hadoop Text Input Formats.\nDue to [MAPREDUCE-232](https://issues.apache.org/jira/browse/MAPREDUCE-232), files created\nin other charsets must be converted to UTF-8 before being processed. This plugin supports\nany character set listed under ``java.nio`` in \nthe [Java Supported Encodings Documentation](https://docs.oracle.com/javase/8/docs/technotes/guides/intl/encoding.doc.html).\n\nUse Case\n--------\nThis action is used whenever you have any text file that is using a character set other than UTF-8.\n\nProperties\n----------\n| Configuration | Required | Default | Description |\n| :------------ | :------: | :------ | :---------- |\n| **Source Path** | **Y** | None | The full path of the file or directory that is to be converted. In the case of a directory, if fileRegex is set, then only files in the source directory matching the regex expression will be moved. Otherwise, all files in the directory will be moved. For example: `hdfs://hostname/tmp`. You can use globbing syntax here. |\n| **Destination Path** | **Y** | None | The full path where the file or files are to be saved. If a directory is specified the files will be created in that directory. If the Source Path is a directory, it is assumed that Destination Path is also a directory. The new files will have ``.utf8`` appended to the end. Files with the same name will be overwritten. |\n| **File Regular Expression** | **N** | None | Regular expression to filter the files in the source directory that will be moved. This is useful when the globbing syntax in the source directory is not precise enough for your files. |\n| **Character Set** | **Y** | None| The name of the character set used to create the file. The complete list of supported character sets can be found in the [Java Supported Encodings Documentation](https://docs.oracle.com/javase/8/docs/technotes/guides/intl/encoding.doc.html). |\n| **Continue Processing If There Are Errors?** | **Y** | false | Indicates if the pipeline should continue if processing the files fails. |\n\nUsage Notes\n-----------\n\nThis plugin can be very useful for converting sets of files to UTF-8 prior to processing them in MapReduce or Spark. Because this action runs as a single process prior to the MapReduce or Spark job, it can take a considerable amount of time to convert large files. If that is the case, you may be better off writing a custom Input format for handling that data.\n \nThe files are created in the destination folder with ``.utf8`` appended to them. Most likely, you will want to use the ``HDFSDelete`` action at the end of the pipeline to clean up these files.\n\nWhen using the Wrangler tool, you will want to wrangle with the UTF-8 converted file to get the best results.\n"
  }
}