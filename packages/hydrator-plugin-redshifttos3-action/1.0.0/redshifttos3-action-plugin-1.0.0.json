{
  "parents": [
    "system:cdap-etl-batch[4.0.0,10.0.0-SNAPSHOT)",
    "system:cdap-etl-realtime[4.0.0,10.0.0-SNAPSHOT)",
    "system:cdap-data-pipeline[4.0.0,10.0.0-SNAPSHOT)",
    "system:cdap-data-streams[4.0.0,10.0.0-SNAPSHOT)"
  ],
  "properties": {
    "widgets.RedshiftToS3-action": "{\"metadata\":{\"spec-version\":\"1.0\"},\"configuration-groups\":[{\"label\":\"DynamoDB Configuration\",\"properties\":[{\"widget-type\":\"password\",\"label\":\"Access Key\",\"name\":\"accessKey\"},{\"widget-type\":\"password\",\"label\":\"Secret Access Key\",\"name\":\"secretAccessKey\"},{\"widget-type\":\"password\",\"label\":\"AWS IAM Role\",\"name\":\"iamRole\"},{\"widget-type\":\"textbox\",\"label\":\"Query\",\"name\":\"query\"},{\"widget-type\":\"textbox\",\"label\":\"S3 Data Path\",\"name\":\"s3DataPath\"},{\"widget-type\":\"textbox\",\"label\":\"Output Path Token\",\"name\":\"outputPathToken\"},{\"widget-type\":\"select\",\"label\":\"Manifest\",\"name\":\"manifest\",\"widget-attributes\":{\"values\":[\"true\",\"false\"],\"default\":\"false\"}},{\"widget-type\":\"textbox\",\"label\":\"S3 Delimiter\",\"name\":\"delimiter\"},{\"widget-type\":\"select\",\"label\":\"Parallel\",\"name\":\"parallel\",\"widget-attributes\":{\"values\":[\"true\",\"false\"],\"default\":\"true\"}},{\"widget-type\":\"select\",\"label\":\"Compression\",\"name\":\"compression\",\"widget-attributes\":{\"values\":[\"NONE\",\"GZIP\",\"BZIP2\"],\"default\":\"NONE\"}},{\"widget-type\":\"select\",\"label\":\"Allow Overwrite\",\"name\":\"allowOverWrite\",\"widget-attributes\":{\"values\":[\"true\",\"false\"],\"default\":\"false\"}},{\"widget-type\":\"select\",\"label\":\"Add Quotes\",\"name\":\"addQuotes\",\"widget-attributes\":{\"values\":[\"true\",\"false\"],\"default\":\"false\"}},{\"widget-type\":\"select\",\"label\":\"Escape\",\"name\":\"escape\",\"widget-attributes\":{\"values\":[\"true\",\"false\"],\"default\":\"false\"}},{\"widget-type\":\"textbox\",\"label\":\"JDBC Redshift Cluster Database URL\",\"name\":\"redshiftClusterURL\"},{\"widget-type\":\"textbox\",\"label\":\"Redshift Master User\",\"name\":\"redshiftMasterUser\"},{\"widget-type\":\"password\",\"label\":\"Redshift Master Password\",\"name\":\"redshiftMasterPassword\"}]}],\"outputs\":[]}",
    "doc.RedshiftToS3-action": "# Redshift To S3 Action\n\n\nDescription\n-----------\nRedshiftToS3 Action that unloads the result of a query from redshift to one or more files on Amazon Simple Storage\nService (Amazon S3).\n\n\nUse Case\n--------\nThis sink is used whenever you need to unload data from Redshift to S3 bucket.\nFor example, a financial customer would like to quickly unload financial reports into S3 that have been generated from\nprocessing that is happening in Redshift. The pipeline would have a Redshift to S3 action at the beginning,\nand then leverage the s3 source to read that data into a processing pipeline.\n\n\nProperties\n----------\n\n**accessKey:** The access Id provided by AWS to access S3 bucket. Both configurations 'Keys(Access and Secret Access\nkeys)' and 'IAM Role' can not be provided or empty at the same time.(Macro-enabled)\n\n**secretAccessKey:** AWS secret access key secret required to access S3 bucket. Both configurations 'Keys(Access and\nSecret Access keys)' and 'IAM Role' can not be provided or empty at the same time.(Macro-enabled)\n\n**iamRole:** IAM role having GET,LIST and PUT permissions to the S3 bucket. The IAM Role can be used only if the cluster\ncorresponding to the ``s3DataPath`` is being hosted on AWS servers. Both configurations 'Keys(Access and Secret Access\nkeys)' and 'IAM Role' can not be provided or empty at the same time.(Macro-enabled)\n\n**query:** A SELECT query, the results of which query are unloaded from Redshift table to the S3 bucket.\n\n**s3DataPath:** The full path, including bucket name, to the location on Amazon S3 where Amazon Redshift will\nwrite the output file objects, including the manifest file if MANIFEST is specified. Should be of the format:\n``s3://object-path/name-prefix``.\n\n**outputPathToken:** The key used to store the S3 file path for the unloaded file, that will be used later by the source\n to read the data from. Plugins that run at later stages in the pipeline can retrieve the file path using this key\nthrough macro substitution:${filePath} where ``filePath`` is the key specified. Defaults to ``filePath``. (Macro-enabled)\n\n**manifest:** Boolean value to determine if manifest file is to be created during unload. The manifest file explicitly\nlists the data files that are created by the UNLOAD process. Default is false\n\n**delimiter:** Single ASCII character that is used to separate fields in the output file. Deafult is pipe(|).\n\n**parallel:** Boolean value to determine if UNLOAD writes data in parallel to multiple files, according to the number\nof slices in the cluster. Default is true.\n\n**compression:** Unloads data into one or more compressed files of type GZIPor BZIP2. Can be one of the following: NONE,\n BZIP2 or GZIP. Default is NONE.\n\n**allowOverWrite:** Boolean value to determine if UNLOAD will overwrite existing files, including the manifest file, if\nthe file is already available. Default is false.\n\n**addQuotes:** Boolean value to determine if UNLOAD places quotation marks around each unloaded data field, so that\nAmazon Redshift can unload data values that contain the delimiter itself. Default is false.\n\n**escape:** Boolean value to determine if escape character(\\) is to be placed before CHAR and VARCHAR columns in\ndelimited unload files, for every occurrence of the following characters: Linefeed ``\\n``, Carriage return ``\\r``,\ndelimiter character specified for the unloaded data, escape character \\ and quote character: \" or '. Default is false.\n\n**redshiftClusterURL:** JDBC Redshift DB url for connecting to the redshift cluster. The url should include the port\nand db name. Should be if format: ``jdbc:redshift://<endpoint-address>:<endpoint-port>/<db-name>``. (Macro-enabled)\n\n**redshiftMasterUser:** Master user for the Redshift cluster to connect to. (Macro-enabled)\n\n**redshiftMasterPassword:** Master password for Redshift cluster to connect to. (Macro-enabled)\n\n\nConditions\n----------\n1. Both configurations 'Keys(Access and Secret Access keys)' and 'IAM Role' can not be provided or empty at the same time.\nEither keys ``(accessKey and secretAccessKey)`` or IAM Role ``(iamRole)`` must be specified for connecting to the S3 bucket.\n\n2. Redshift table from which user wants to unload the data, should already exist in the database, in the Redshift cluster\nspecified by the ``redshiftClusterURL``.\n\n3. The Amazon S3 bucket where Amazon Redshift will write the output files must reside in the same region as your cluster.\n\n4. S3 data path should start with ``s3://`` and not with the ``s3n://`` or ``s3a://`` URI scheme.\n\nExample\n-------\nThis example connects to a Redshift cluster using the ``redshiftClusterURL, redshiftMasterUser and redshiftMasterPassword``\nand to the S3 instance using the credentials ``accessKey`` and ``secretAccessKey``. Result of the query will be unloaded\nto the S3 bucket provided through ``s3DataPath``.\n\n    {\n        \"name\": \"RedshiftToS3Action\",\n        \"type\": \"action\",\n        \"properties\": {\n            \"accessKey\": \"accessKey\",\n            \"secretAccessKey\": \"secretAccessKey\",\n            \"iamRole\": \"\",\n            \"query\": \"select * from venue\",\n            \"s3DataPath\": \"s3://mybucket/test_\",\n            \"outputPathToken\": \"\",\n            \"manifest\": \"false\",\n            \"delimiter\": \",\",\n            \"parallel\": \"false\",\n            \"compression\" : \"NONE\",\n            \"allowOverwrite\" : \"true\",\n            \"addQuotes\": \"true\",\n            \"escape\": \"false\",\n            \"redshiftClusterURL\" : \"jdbc:redshift://x.y.us-west-1.redshift.amazonaws.com:5439/redshiftdb\",\n            \"redshiftMasterUser\" : \"admin\",\n            \"redshiftMasterPassword\": \"admin\"\n        }\n    }"
  }
}